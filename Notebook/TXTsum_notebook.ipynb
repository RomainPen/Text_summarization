{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation summarization project (BART and T5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train : set for training (train part), 100% of conv_train.parquet\n",
    "df_val1 : set for training (val part), 100% of conv_val.parquet\n",
    "df_val2 : set for model validation, 100% of conv_test.parquet\n",
    "\n",
    "df_val2_test (imported later): same set of the df_val2, but use for deployment testing, 100% of conv_test.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0/ Config Google colab and Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Config Google_colab and Google_drive :\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# #change directory :\n",
    "# os.chdir(r'./drive/MyDrive/Data_science/PROJET_perso/NLP_project/Notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\romai\\\\Bureau\\\\DATA_SCIENCE\\\\PROJET_perso\\\\NLP\\\\Text_Summarization\\\\Text_summarization\\\\Notebook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\nJ...</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia: Who are you voting for in this electio...</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Tim: Hi, what's up?\\nKim: Bad mood tbh, I was ...</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>Sam: hey  overheard rick say something\\nSam: i...</td>\n",
       "      <td>Sam is confused, because he overheard Rick com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14727</th>\n",
       "      <td>13863028</td>\n",
       "      <td>Romeo: You are on my â€˜People you may knowâ€™ lis...</td>\n",
       "      <td>Romeo is trying to get Greta to add him to her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14728</th>\n",
       "      <td>13828570</td>\n",
       "      <td>Theresa: &lt;file_photo&gt;\\nTheresa: &lt;file_photo&gt;\\n...</td>\n",
       "      <td>Theresa is at work. She gets free food and fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14729</th>\n",
       "      <td>13819050</td>\n",
       "      <td>John: Every day some bad news. Japan will hunt...</td>\n",
       "      <td>Japan is going to hunt whales again. Island an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14730</th>\n",
       "      <td>13828395</td>\n",
       "      <td>Jennifer: Dear Celia! How are you doing?\\nJenn...</td>\n",
       "      <td>Celia couldn't make it to the afternoon with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14731</th>\n",
       "      <td>13729017</td>\n",
       "      <td>Georgia: are you ready for hotel hunting? We n...</td>\n",
       "      <td>Georgia and Juliette are looking for a hotel i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14732 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           dialogue  \\\n",
       "0      13818513  Amanda: I baked  cookies. Do you want some?\\nJ...   \n",
       "1      13728867  Olivia: Who are you voting for in this electio...   \n",
       "2      13681000  Tim: Hi, what's up?\\nKim: Bad mood tbh, I was ...   \n",
       "3      13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
       "4      13728094  Sam: hey  overheard rick say something\\nSam: i...   \n",
       "...         ...                                                ...   \n",
       "14727  13863028  Romeo: You are on my â€˜People you may knowâ€™ lis...   \n",
       "14728  13828570  Theresa: <file_photo>\\nTheresa: <file_photo>\\n...   \n",
       "14729  13819050  John: Every day some bad news. Japan will hunt...   \n",
       "14730  13828395  Jennifer: Dear Celia! How are you doing?\\nJenn...   \n",
       "14731  13729017  Georgia: are you ready for hotel hunting? We n...   \n",
       "\n",
       "                                                 summary  \n",
       "0      Amanda baked cookies and will bring Jerry some...  \n",
       "1      Olivia and Olivier are voting for liberals in ...  \n",
       "2      Kim may try the pomodoro technique recommended...  \n",
       "3      Edward thinks he is in love with Bella. Rachel...  \n",
       "4      Sam is confused, because he overheard Rick com...  \n",
       "...                                                  ...  \n",
       "14727  Romeo is trying to get Greta to add him to her...  \n",
       "14728  Theresa is at work. She gets free food and fre...  \n",
       "14729  Japan is going to hunt whales again. Island an...  \n",
       "14730  Celia couldn't make it to the afternoon with t...  \n",
       "14731  Georgia and Juliette are looking for a hotel i...  \n",
       "\n",
       "[14732 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data (training part) : \n",
    "\n",
    "df_train = pd.read_parquet(r\"..\\Data\\training_data\\conv_train.parquet\", engine=\"pyarrow\")\n",
    "df_val1 = pd.read_parquet(r\"..\\Data\\training_data\\conv_val1.parquet\", engine=\"pyarrow\")\n",
    "df_val2 = pd.read_parquet(r\"..\\Data\\training_data\\conv_val2.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue : \n",
      "Oli: I've talked to some people from the third year\n",
      "Jacob: About the statistics exam?\n",
      "Marcia: What did they say?\n",
      "Oli: Yeah, about the exam\n",
      "Oli: We need to prepare for a battle\n",
      "Jacob: So it will be difficult\n",
      "Oli: They said it was the hardest exam ever\n",
      "Marcia: ðŸ˜±\n",
      "Oli: The questions were displayed on the screen \n",
      "Oli: One minute per question and it disappears\n",
      "Oli: They won't come back so if you didn't get your answer you're fucked\n",
      "Marcia: So we need to make the calculations really fast\n",
      "Jacob: That's insane\n",
      "Oli: I know\n",
      "Oli: Very stressful\n",
      "Marcia: How are we even supposed to study for it?\n",
      "Marcia: With a timer?\n",
      "Oli: I guess\n",
      "Marcia: Did anybody pass it last year\n",
      "Oli: Some people did, but the majority had to take the second or even the third chance\n",
      " \n",
      "summary : \n",
      "Oli, Jacob and Marcia have to prepare for a very hard statistics exam. Last year, people had only one minute to answer each question and then it disappeared.\n"
     ]
    }
   ],
   "source": [
    "# Analyse some examples :\n",
    "def print_example(df, txt_id) : \n",
    "    print(f\"dialogue : \\n{df[\"dialogue\"].loc[txt_id]}\")\n",
    "    print(\" \")\n",
    "    print(f\"summary : \\n{df[\"summary\"].loc[txt_id]}\")\n",
    "\n",
    "print_example(df_train, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/ Split df into df_train/df_val1/df_val2 : (already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1/ Data cleaning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename col : df_train.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n",
    "col_to_rename = {}\n",
    "\n",
    "# Check and change col type (with new col name):\n",
    "col_type = {\"id\" : \"object\", \"dialogue\" : \"object\", \"summary\" : \"object\"}\n",
    "\n",
    "# Useless columns :\n",
    "useless_columns = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning function : \n",
    "def data_cleaning_1(df, col_to_rename, col_type, useless_columns) :\n",
    "    # rename col :\n",
    "    df_new = df.rename(columns=col_to_rename)\n",
    "\n",
    "    # check and check col type :\n",
    "    for col in df_new.columns : \n",
    "        df_new[col] = df_new[col].astype(col_type[col])\n",
    "    \n",
    "    #remove duplicates :\n",
    "    df_new = df_new.drop_duplicates()\n",
    "    df_new = df_new.reset_index(drop=True)\n",
    "\n",
    "    #remove useless columns :\n",
    "    df_new = df_new.drop(list(useless_columns), axis=1)\n",
    "\n",
    "    # Remove row where there is NaN in \"dialogue\" or in \"summary\" :\n",
    "    df_new = df_new.dropna(subset=[\"dialogue\", \"summary\"], ignore_index=True)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "kwarg_dc1 = {\"col_to_rename\": col_to_rename, \"col_type\": col_type, \"useless_columns\":useless_columns}\n",
    "df_train = data_cleaning_1(df=df_train, **kwarg_dc1)\n",
    "df_val1 = data_cleaning_1(df=df_val1, **kwarg_dc1)\n",
    "df_val2 = data_cleaning_1(df=df_val2, **kwarg_dc1)\n",
    "\n",
    "\n",
    "# Check Language consistency (english txt == english summary), by hand : #Already done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2/ Data cleaning 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary'],\n",
       "    num_rows: 14731\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert df (pandas df format) to Dataset (huggingface format) :\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val1 = Dataset.from_pandas(df_val1)\n",
    "dataset_val2 = Dataset.from_pandas(df_val2)\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14731\n",
       "    })\n",
       "    val1: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "    val2: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat \"dataset_train, dataset_val1, dataset_val2\" into a DatasetDict : \n",
    "from datasets import DatasetDict\n",
    "Datadict = DatasetDict()\n",
    "\n",
    "Datadict['train'] = dataset_train\n",
    "Datadict['val1'] = dataset_val1\n",
    "Datadict['val2'] = dataset_val2\n",
    "\n",
    "Datadict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of data processing function : \n",
    "\n",
    "# # example 1 : using function\n",
    "# # 2 (or more) process in one : \n",
    "# def process_example(example):\n",
    "#     summary_lower = [x.lower() for x in example[\"summary\"]]\n",
    "#     summary_length = [len(x.split()) for x in example[\"summary\"]]\n",
    "#     return {\"summary_lower\": summary_lower, \"summary_length\":summary_length}\n",
    "\n",
    "# Datadict = Datadict.map(process_example, batched=True)\n",
    "# Datadict.set_format(\"pandas\")\n",
    "# Datadict[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example 2 : using lambda\n",
    "# # We use batched=True when we are doing a list comprehension and when the output value is a list.\n",
    "# # And when we do the tokenization\n",
    "\n",
    "# Datadict = Datadict.map(lambda x: {\"abc_list\": [i.split(\"o\") for i in x[\"abc\"]]}, batched=True)\n",
    "# Datadict.set_format(\"pandas\")\n",
    "# Datadict[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b98244269214e0f8c2f26d62b2c5b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c115a889961245c08929a5d3ab41cea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd57272477742f1a26d47a3ad9618f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalized txt :\n",
    "import html\n",
    "import re\n",
    "\n",
    "\n",
    "def normalized_txt(df):\n",
    "    processed_dialogue = []\n",
    "    processed_summary = []\n",
    "    \n",
    "    for text in df[\"dialogue\"]:\n",
    "        text = process_text(text)\n",
    "        processed_dialogue.append(text)\n",
    "    \n",
    "    for text in df[\"summary\"]:\n",
    "        text = process_text(text)\n",
    "        processed_summary.append(text)\n",
    "    \n",
    "    return {\"dialogue\": processed_dialogue, \"summary\": processed_summary}\n",
    "\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    # text = text.lower()\n",
    "    \n",
    "    # Remove extra spaces at the start and end\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Replace control characters (\\r, \\n, \\t) with ' | '\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' | ', text)\n",
    "    \n",
    "    # Remove redundant spaces within the text\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # STANDARDIZE PUNCTUATION :\n",
    "    # Convert different types of quotation marks and apostrophes to standard forms\n",
    "    text = text.replace('â€œ', '\"').replace('â€', '\"')\n",
    "    text = text.replace(\"â€˜\", \"'\").replace(\"â€™\", \"'\")\n",
    "    \n",
    "    # Ensure consistent spacing around punctuation\n",
    "    text = re.sub(r'([?.!,%Â£$â‚¬\"])', r'\\1 ', text)  # Remove spaces before punctuation and Ensure space after punctuation (except at end of string)\n",
    "    text = re.sub(r'([&:+-=;])', r' \\1 ', text)\n",
    "    \n",
    "    # Convert special punctuation to standard forms (example: em dash to hyphen)\n",
    "    #text = text.replace('â€”', '-').replace('â€“', '-')\n",
    "    \n",
    "    # REMOVE or REPLACE USELESS CHARACTERS :\n",
    "    # Remove URLs\n",
    "    #text = re.sub(r'http\\S+|www\\S+|https\\S+', '<Web_site_link>', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    #text = re.sub(r'\\S+@\\S+', '<email_address>', text)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the combined function to the dataset\n",
    "Datadict = Datadict.map(normalized_txt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct spelling and grammar : # Already done\n",
    "# remove noise (jargon, slang, typo) : # Not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de7e6388a3f4268b78d36e8f3256e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed47fdf403348e5a5e2b08a834d231b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffbe3f0c0444c478105f416a4b84b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Handle Emoji :\n",
    "import emoji\n",
    "Datadict = Datadict.map(lambda df: {\"dialogue\": [emoji.demojize(txt) for txt in df[\"dialogue\"]]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda :  I baked cookies .   Do you want some...</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia :  Who are you voting for in this elect...</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Tim :  Hi ,   what's up?  | Kim :  Bad mood tb...</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13818513  Amanda :  I baked cookies .   Do you want some...   \n",
       "1  13728867  Olivia :  Who are you voting for in this elect...   \n",
       "2  13681000  Tim :  Hi ,   what's up?  | Kim :  Bad mood tb...   \n",
       "\n",
       "                                             summary  \n",
       "0  Amanda baked cookies and will bring Jerry some...  \n",
       "1  Olivia and Olivier are voting for liberals in ...  \n",
       "2  Kim may try the pomodoro technique recommended...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# > Check for consistency (by hand and map) : # Done\n",
    "# > Consistency in Formatting (by hand and map) : # Done\n",
    "Datadict.set_format(\"pandas\")\n",
    "Datadict[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset to arrow format :\n",
    "Datadict.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filter (select specific text, if necessary) : # Already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8676643177ef447e9ac2467da4abb4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1265686bbea1434e80d1623f564231e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3dc3d2722a45c992a821ea932df8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the distribution of words in the text and their reference summary (If ref_summary to Short/Long, then delete this row, else keep it) : # everything is good\n",
    "\n",
    "Datadict = Datadict.map(lambda df : {\"len_dialogue\" : [len(txt) for txt in df[\"dialogue\"]], \"len_summary\" : [len(txt) for txt in df[\"summary\"]]}, batched=True)\n",
    "Datadict.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Datadict[\"train\"][\"len_dialogue\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Datadict[\"train\"][\"len_summary\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset to arrow format :\n",
    "Datadict.reset_format()\n",
    "\n",
    "# Delete col [\"len_dialogue\", \"len_summary\"] : \n",
    "Datadict = Datadict.remove_columns([\"len_dialogue\", \"len_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5/ Choose the model (BART or T5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "T5 or T5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model checkpoint :\n",
    "model_checkpoint = \"google-t5/t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the devise (cuda or CPU) :\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6/ Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization :\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 200\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"dialogue\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True)\n",
    "\n",
    "    labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]  # output_ids <=> labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b10f2feb51647f9bfaa5a83b381adc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134ddf1efdf9498998ad4a114f9de932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bce15cde4f4b1b9849b2ab4d15d5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14731\n",
       "    })\n",
       "    val1: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "    val2: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = Datadict.map(preprocess_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7/ Metrics for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge-score :\n",
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.923076923076923,\n",
       " 'rouge2': 0.7272727272727272,\n",
       " 'rougeL': 0.923076923076923,\n",
       " 'rougeLsum': 0.923076923076923}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of usage :\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "\n",
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary], use_stemmer=True\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923076923076923"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"rouge1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8/ Create the baseline model and evaluate the score of the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\romai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Create the baseline summary :\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oli :  I've talked to some people from the third year | Jacob :  About the statistics exam?\n",
      "| Marcia :  What did they say?\n",
      "| Oli :  Yeah ,   about the exam | Oli :  We need to prepare for a battle | Jacob :  So it will be difficult | Oli :  They said it was the hardest exam ever | Marcia :   | Oli :  The questions were displayed on the screen | Oli :  One minute per question and it disappears | Oli :  They won't come back so if you didn't get your answer you're fucked | Marcia :  So we need to make the calculations really fast | Jacob :  That's insane | Oli :  I know | Oli :  Very stressful | Marcia :  How are we even supposed to study for it?\n"
     ]
    }
   ],
   "source": [
    "# example of usage : \n",
    "print(three_sentence_summary(tokenized_datasets[\"train\"][60][\"dialogue\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm romain.\n",
      "you are paul.\n",
      "We are a team.\n"
     ]
    }
   ],
   "source": [
    "# Example 2 : \n",
    "print(three_sentence_summary(\"I'm romain. you are paul. We are a team. yohou!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate baseline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate baseline on validation set (val1) :\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries_baseline = [three_sentence_summary(text) for text in dataset[\"dialogue\"]]\n",
    "    return metric.compute(predictions=summaries_baseline, references=dataset[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 30.28, 'rouge2': 9.9, 'rougeL': 23.82, 'rougeLsum': 26.29}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "score = evaluate_baseline(tokenized_datasets[\"val1\"], rouge_score)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict((rn, round(score[rn] * 100, 2)) for rn in rouge_names)\n",
    "rouge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9/ Fine-tuned the T5-small model (with TrainerAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model :\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Attribute the devise to the model :\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Optional] :\n",
    "# # Train model with LoRA :\n",
    "# from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "\n",
    "\n",
    "# peft_config = LoraConfig(task_type=\"SEQ_2_SEQ_LM\",\n",
    "#                         r=4,\n",
    "#                         lora_alpha=32,\n",
    "#                         lora_dropout=0.01,\n",
    "#                         target_modules = ['q_lin'])\n",
    "\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba1119be49e463aae7d887c29299040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notebook login :\n",
    "# Use it for saving the model in the hub :\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training argument:\n",
    "from transformers import Seq2SeqTrainingArguments, TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"..\\\\Model\\\\{model_name}_finetuned_samsum_en\", #checkpopint path\n",
    "    eval_strategy=\"epoch\", # or \"steps\"\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the evaluation function :\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores # to review : value.mid.fmeasure <=> value\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data collator for the sequence-to-sequence task :\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [21542, 3, 10, 27, 13635, 5081, 3, 5, 531, 25, 241, 128, 58, 1820, 16637, 3, 10, 10625, 55, 1820, 21542, 3, 10, 27, 31, 195, 830, 25, 5721, 3, 10, 3, 18, 3, 61, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [21542, 13635, 5081, 11, 56, 830, 16637, 128, 5721, 3, 5, 1]}, {'input_ids': [25051, 3, 10, 2645, 33, 25, 10601, 21, 16, 48, 4356, 58, 1820, 15865, 3, 10, 18587, 7, 38, 373, 3, 5, 1820, 25051, 3, 10, 1212, 396, 55, 3, 55, 1820, 15865, 3, 10, 1651, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [25051, 11, 20373, 5144, 33, 10601, 21, 10215, 7, 16, 48, 4356, 3, 5, 1]}]\n"
     ]
    }
   ],
   "source": [
    "# Example of data collator :\n",
    "tokenized_datasets2 = tokenized_datasets.remove_columns(Datadict[\"train\"].column_names)\n",
    "\n",
    "features = [tokenized_datasets2[\"train\"][i] for i in range(2)]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[21542,     3,    10,    27, 13635,  5081,     3,     5,   531,    25,\n",
       "           241,   128,    58,  1820, 16637,     3,    10, 10625,    55,  1820,\n",
       "         21542,     3,    10,    27,    31,   195,   830,    25,  5721,     3,\n",
       "            10,     3,    18,     3,    61,     1,     0],\n",
       "        [25051,     3,    10,  2645,    33,    25, 10601,    21,    16,    48,\n",
       "          4356,    58,  1820, 15865,     3,    10, 18587,     7,    38,   373,\n",
       "             3,     5,  1820, 25051,     3,    10,  1212,   396,    55,     3,\n",
       "            55,  1820, 15865,     3,    10,  1651,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[21542, 13635,  5081,    11,    56,   830, 16637,   128,  5721,     3,\n",
       "             5,     1,  -100,  -100,  -100],\n",
       "        [25051,    11, 20373,  5144,    33, 10601,    21, 10215,     7,    16,\n",
       "            48,  4356,     3,     5,     1]]), 'decoder_input_ids': tensor([[    0, 21542, 13635,  5081,    11,    56,   830, 16637,   128,  5721,\n",
       "             3,     5,     1,     0,     0],\n",
       "        [    0, 25051,    11, 20373,  5144,    33, 10601,    21, 10215,     7,\n",
       "            16,    48,  4356,     3,     5]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08733c29926b40b7824523d681312a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 15\u001b[0m\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      5\u001b[0m     model,\n\u001b[0;32m      6\u001b[0m     args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Launch the train :\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1703\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[1;32m-> 1703\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1704\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[0;32m   1713\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1714\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1715\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1716\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1717\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1107\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1093\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1094\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         output_attentions,\n\u001b[0;32m   1105\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:747\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    744\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:336\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    335\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 336\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:282\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    281\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi(hidden_states)\n\u001b[1;32m--> 282\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[0;32m    288\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Bureau\\DATA_SCIENCE\\PROJET_perso\\NLP\\Text_Summarization\\Text_summarization\\txtsum_env\\Lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instanciate the trainer (with the standard arguments):\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val1\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Launch the train :\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the val1 set :\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ROUGE score interpretation :\n",
    "> ROUGE-1 scores are excellent around 0.5, with scores above 0.5 considered good and 0.4 to 0.5 moderate. \n",
    "> ROUGE-2 scores above 0.4 are good, and 0.2 to 0.4 are moderate. \n",
    "> ROUGE-L scores are good around 0.4 and low at 0.3 to 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/ Valid the model by evaluating the model on the test set (val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/ Save the model (localy) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model :\n",
    "trainer.save_model(r'..\\Model\\T5_small_finetune_samsum_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/ Analyse the model by testing on ONE example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Optional]\n",
    "# # Load model with PEFT :\n",
    "\n",
    "# model_checkpoint_path = f\"..\\\\Model\\\\{model_name}_finetuned_samsum_en\" \n",
    "# model_path = './T5_small_finetune_samsum_model'\n",
    "# config = PeftConfig.from_pretrained(model_checkpoint_path)\n",
    "# inference_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = PeftModel.from_pretrained(inference_model, model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('summarization', model='./T5_small_finetune_samsum_model')\n",
    "gen_kwargs = {'length_penalty': 0.8, 'num_beams': 8, \"max_length\": 20}\n",
    "\n",
    "custom_dialogue=\"\"\"\n",
    "Romain: Hey ! Can you help me to clear the room ? \n",
    "Carla: Yes I can. Could you wait me a minute ? \n",
    "Romain: Yes of course, I will tidy the table first. \n",
    "Carla: Ok ! \n",
    "\"\"\"\n",
    "print(pipe(custom_dialogue, **gen_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13/ Deploy the model in real situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14/ Check if the Deployment is good"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Compute the rouge_score of df_val2_brut (output of deployment pipeline)\n",
    "Compare this score with the rouge_score of df_val2 (df used during the training)\n",
    "If df_val2_brut rouge_score == df_val2 rouge_score : GOOD\n",
    "else : PROBLEM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txtsum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
